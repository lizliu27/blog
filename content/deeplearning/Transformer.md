+++
title = 'Seq2Seq, RNN, LSTM, Transformer '
date = 2024-03-28T14:22:17+08:00
draft = false
+++

## Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks.

A summary of the tasks and techniques covered:

### RNN and LSTM Implementation
- Implementing a vanilla `RNN` unit using PyTorch Linear layers and activations.
- Implementing an `LSTM` unit using PyTorch nn.Parameter and activations, following a set of equations.

### Seq2Seq Implementation
- Implementing Seq2Seq models with an `encoder` and `decoder`.

### Seq2Seq with Attention
- Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.
- Referencing research papers for deeper understanding of attention mechanisms.

### Transformers
- Implementing a one-layer `transformer encoder`.
- Tasks include embeddings, multi-head self-attention, element-wise feedforward layer, final layer, forward pass, and training.
- Training the transformer encoder architecture on the dataset with default hyperparameters.

### Full Transformer Implementation
- Implementing a full transformer model using PyTorch built-in modules.
- Training the model with hyper-parameter tuning and comparing results with other implemented models.


Project 4 provides hands-on experience with building and training various neural network architectures for natural language processing tasks. Participants learn about RNNs, LSTMs, attention mechanisms, and transformers, gaining insights into their functionalities and applications in NLP. Additionally, participants gain experience in model implementation, training, and evaluation, along with hyperparameter tuning and result analysis.

Overall, Project 4 serves as a comprehensive exploration of advanced NLP techniques, equipping participants with practical skills and knowledge in deep learning for sequence modeling and language translation tasks.
