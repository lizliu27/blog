<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><meta charset=utf-8><title>Liz Liu
</title><meta name=generator content="Hugo 0.125.6"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Liz Liu"><meta name=description content="Machine Learning & Life
"><link rel=stylesheet href=/blog/scss/main.min.009f917038f30ebd1f2147e8e1dfd40fc1b799422a7869aa0da12af1fd1bf8ba.css integrity="sha256-AJ+RcDjzDr0fIUfo4d/UD8G3mUIqeGmqDaEq8f0b+Lo=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/blog/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/blog/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=canonical href=https://lizliu27.github.io/blog/><link rel=alternate type=application/rss+xml href=/blog/index.xml title><script type=text/javascript src=/blog/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script><script type=text/javascript src=/blog/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Machine Learning & Life
"><meta property="og:url" content="https://lizliu27.github.io/blog/"><meta property="og:site_name" content="Liz Blog"><meta property="og:title" content="Liz Blog"><meta property="og:description" content="Machine Learning & Life"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"","url":"https:\/\/lizliu27.github.io\/blog\/","description":"Machine Learning \u0026 Life\n","thumbnailUrl":"https:\/\/lizliu27.github.io\/blog\/","license":""}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/blog/images/avatar.jpg alt="profile picture"><h1 class=sidebar__introduction-title><a href=/blog>Liz Blog</a></h1><div class=sidebar__introduction-description><p>Machine Learning & Life<br></p></div></div><ul class=sidebar__list></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Liz Liu
2024</li></ul></footer><script type=text/javascript src=/blog/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a class=nav__link--active href=/blog/ title>Home</a></li><li class=nav__list-item><a href=/blog/machinelearning/ title>Machine Learning for Trading</a></li><li class=nav__list-item><a href=/blog/about/ title>About</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content></div></div><div class="post
animated fadeInDown"><div class=post__content><h3><a href=/blog/deeplearning/transformer/>SEQ2SEQ, RNN, LSTM, TRANSFORMER</a></h3><p>Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:
RNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.</p></div><div class=post__footer><em class="fas fa-calendar-day"></em>
<span class=post__footer-date>Thu, Mar 28, 2024</span></div></div><div class="post
animated fadeInDown"><div class=post__content><h3><a href=/blog/deeplearning/eeg/>DEEP LEARNING IN AUTO-DIAGNOSIS ELECTROENCEPHALOGRAPHY</a></h3><p>This project was led by my awesome teammate zheng cheng. We together replicated a result an exsiting paper.
We tried to replicate the results and applied deep learning models to the auto-diagnosis of epilepsy via Electroencephalography (EEG) profiles.
We developed an architecture based on transformers, similar to those used for understanding sentiment in text. This architecture helped us extract important information from our data. We made several adjustments to this architecture and fine-tuned its settings.</p></div><div class=post__footer><em class="fas fa-calendar-day"></em>
<span class=post__footer-date>Fri, Dec 22, 2023</span></div></div><div class="post
animated fadeInDown"><div class=post__content><h3><a href=/blog/deeplearning/cnn/>CNN</a></h3><p>Project 2: Training Convolutional Neural Network (CNN) from scratch Module Implementation:
I learned to build a two-layer network with fully connected layers and a sigmoid activation function. I implemented a vanilla CNN with a convolutional layer, ReLU activation, max-pooling layer, and fully connected layer for classification. I had the opportunity to design and build my own custom CNN model, adhering to the principles of network architecture design. CNN Model Summary</p></div><div class=post__footer><em class="fas fa-calendar-day"></em>
<span class=post__footer-date>Tue, Nov 28, 2023</span></div></div><div class="post
animated fadeInDown"><div class=post__content><h3><a href=/blog/deeplearning/saliency_map_style_transfer/>SALIENCY MAPS, GRADCAM AND STYLE TRANSFER</a></h3><p>Project 3: Implementing various techniques related to image analysis and manipulation using deep learning Specifically convolutional neural networks (CNNs) and style transfer. Hereâ€™s a summary of the tasks and techniques covered:
Class Model Visualizations: Synthesizing images to maximize classification scores for specific classes, providing insights into network focus during classification. Generating class-specific saliency maps to understand image areas influencing classification decisions. Creating fooling images by perturbing input images to mislead pretrained networks.</p></div><div class=post__footer><em class="fas fa-calendar-day"></em>
<span class=post__footer-date>Tue, Nov 28, 2023</span></div></div><div class="post
animated fadeInDown"><div class=post__content><h3><a href=/blog/deeplearning/mlp/>TRANING NEURAL NETWORKS</a></h3><p>Project 1: Training Neural Networks for MNIST Recognition The first project focused on building a simple pipeline for training neural networks to recognize hand-written digits from the MNIST dataset. The pipeline implementation encompassed two neural network architectures, each equipped with functionalities to load data, train, and optimize the models.
Neural Network Architectures Two distinct neural network architectures were implemented from scratch for this project:
Simple Softmax Regression: Composed of a fully-connected layer followed by a ReLU activation.</p></div><div class=post__footer><em class="fas fa-calendar-day"></em>
<span class=post__footer-date>Tue, Nov 28, 2023</span></div></div><div class=pagination></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Liz Liu
2024</li></ul></footer><script type=text/javascript src=/blog/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>