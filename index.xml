<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title/><link>https://lizliu27.github.io/blog/</link><description>Recent content on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 14:22:17 +0800</lastBuildDate><atom:link href="https://lizliu27.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Seq2Seq, RNN, LSTM, Transformer</title><link>https://lizliu27.github.io/blog/deeplearning/transformer/</link><pubDate>Thu, 28 Mar 2024 14:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/transformer/</guid><description>Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:
RNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.</description></item><item><title>CNN</title><link>https://lizliu27.github.io/blog/deeplearning/cnn/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/cnn/</guid><description>Project 2: Training Convolutional Neural Network (CNN) from scratch Module Implementation:
I learned to build a two-layer network with fully connected layers and a sigmoid activation function. I implemented a vanilla CNN with a convolutional layer, ReLU activation, max-pooling layer, and fully connected layer for classification. I had the opportunity to design and build my own custom CNN model, adhering to the principles of network architecture design. CNN Model Summary</description></item><item><title>Saliency Maps, GradCAM and Style Transfer</title><link>https://lizliu27.github.io/blog/deeplearning/saliency_map_style_transfer/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/saliency_map_style_transfer/</guid><description>Project 3: Implementing various techniques related to image analysis and manipulation using deep learning Specifically convolutional neural networks (CNNs) and style transfer. Here&amp;rsquo;s a summary of the tasks and techniques covered:
Class Model Visualizations: Synthesizing images to maximize classification scores for specific classes, providing insights into network focus during classification. Generating class-specific saliency maps to understand image areas influencing classification decisions. Creating fooling images by perturbing input images to mislead pretrained networks.</description></item><item><title>Traning Neural Networks</title><link>https://lizliu27.github.io/blog/deeplearning/mlp/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/mlp/</guid><description>Project 1: Training Neural Networks for MNIST Recognition The first project focused on building a simple pipeline for training neural networks to recognize hand-written digits from the MNIST dataset. The pipeline implementation encompassed two neural network architectures, each equipped with functionalities to load data, train, and optimize the models.
Neural Network Architectures Two distinct neural network architectures were implemented from scratch for this project:
Simple Softmax Regression: Composed of a fully-connected layer followed by a ReLU activation.</description></item><item><title>Machine Learning for Trading Project3</title><link>https://lizliu27.github.io/blog/machinelearning/ml4t_summary/</link><pubDate>Wed, 22 Feb 2023 14:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/machinelearning/ml4t_summary/</guid><description>CART models are widely used in machine learning dealing with non-linear relationships and making predictions. To further understand which model is appropriate and the performance of models under different parameters, a series of experiments was performed. For this experiment, the main dataset was currency exchange data collected by Istanbul currency exchange. The 3 experiments were focused on overfitting, the use of bagging and the performance of classic decision tree vs random decision tree.</description></item><item><title>About</title><link>https://lizliu27.github.io/blog/about/</link><pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate><guid>https://lizliu27.github.io/blog/about/</guid><description>Hugo, the world's fastest framework for building websites</description></item></channel></rss>