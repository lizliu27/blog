<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><meta charset=utf-8><title>Liz Liu
|
Seq2Seq, RNN, LSTM, Transformer
</title><meta name=generator content="Hugo 0.125.6"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Liz Liu"><meta name=description content="Machine Learning & Life
"><link rel=stylesheet href=/blog/scss/main.min.009f917038f30ebd1f2147e8e1dfd40fc1b799422a7869aa0da12af1fd1bf8ba.css integrity="sha256-AJ+RcDjzDr0fIUfo4d/UD8G3mUIqeGmqDaEq8f0b+Lo=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/blog/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/blog/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/blog/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=canonical href=https://lizliu27.github.io/blog/deeplearning/transformer/><script type=text/javascript src=/blog/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script><script type=text/javascript src=/blog/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Seq2Seq, RNN, LSTM, Transformer "><meta name=twitter:description content="Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:
RNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance."><meta property="og:url" content="https://lizliu27.github.io/blog/deeplearning/transformer/"><meta property="og:site_name" content="Liz Blog"><meta property="og:title" content="Seq2Seq, RNN, LSTM, Transformer "><meta property="og:description" content="Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:
RNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="deeplearning"><meta property="article:published_time" content="2024-03-28T14:22:17+08:00"><meta property="article:modified_time" content="2024-03-28T14:22:17+08:00"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"deeplearning","name":"Seq2Seq, RNN, LSTM, Transformer ","headline":"Seq2Seq, RNN, LSTM, Transformer ","alternativeHeadline":"","description":"
      
        Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:\nRNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/lizliu27.github.io\/blog\/deeplearning\/transformer\/"},"author":{"@type":"Person","name":"Liz Liu"},"creator":{"@type":"Person","name":"Liz Liu"},"accountablePerson":{"@type":"Person","name":"Liz Liu"},"copyrightHolder":{"@type":"Person","name":"Liz Liu"},"copyrightYear":"2024","dateCreated":"2024-03-28T14:22:17.00Z","datePublished":"2024-03-28T14:22:17.00Z","dateModified":"2024-03-28T14:22:17.00Z","publisher":{"@type":"Organization","name":"Liz Liu","url":"https://lizliu27.github.io/blog/","logo":{"@type":"ImageObject","url":"https:\/\/lizliu27.github.io\/blog\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/lizliu27.github.io\/blog\/deeplearning\/transformer\/","wordCount":"226","genre":[],"keywords":[]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/blog/images/avatar.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/blog>Liz Blog</a></div><div class=sidebar__introduction-description><p>Machine Learning & Life<br></p></div></div><ul class=sidebar__list></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Liz Liu
2024</li></ul></footer><script type=text/javascript src=/blog/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/blog/ title>Home</a></li><li class=nav__list-item><a href=/blog/machinelearning/ title>Machine Learning for Trading</a></li><li class=nav__list-item><a href=/blog/about/ title>About</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Seq2Seq, RNN, LSTM, Transformer</h1><h2 id=project-4-implementing-sequence-to-sequence-seq2seq-models-and-transformers-for-natural-language-processing-tasks>Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks.</h2><p>A summary of the tasks and techniques covered:</p><h3 id=rnn-and-lstm-implementation>RNN and LSTM Implementation</h3><ul><li>Implementing a vanilla <code>RNN</code> unit using PyTorch Linear layers and activations.</li><li>Implementing an <code>LSTM</code> unit using PyTorch nn.Parameter and activations, following a set of equations.</li></ul><h3 id=seq2seq-implementation>Seq2Seq Implementation</h3><ul><li>Implementing Seq2Seq models with an <code>encoder</code> and <code>decoder</code>.</li></ul><h3 id=seq2seq-with-attention>Seq2Seq with Attention</h3><ul><li>Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.</li><li>Referencing research papers for deeper understanding of attention mechanisms.</li></ul><h3 id=transformers>Transformers</h3><ul><li>Implementing a one-layer <code>transformer encoder</code>.</li><li>Tasks include embeddings, multi-head self-attention, element-wise feedforward layer, final layer, forward pass, and training.</li><li>Training the transformer encoder architecture on the dataset with default hyperparameters.</li></ul><h3 id=full-transformer-implementation>Full Transformer Implementation</h3><ul><li>Implementing a full transformer model using PyTorch built-in modules.</li><li>Training the model with hyper-parameter tuning and comparing results with other implemented models.</li></ul><p>Project 4 provides hands-on experience with building and training various neural network architectures for natural language processing tasks. Participants learn about RNNs, LSTMs, attention mechanisms, and transformers, gaining insights into their functionalities and applications in NLP. Additionally, participants gain experience in model implementation, training, and evaluation, along with hyperparameter tuning and result analysis.</p><p>Overall, Project 4 serves as a comprehensive exploration of advanced NLP techniques, equipping participants with practical skills and knowledge in deep learning for sequence modeling and language translation tasks.</p></div><div class=post__footer></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Liz Liu
2024</li></ul></footer><script type=text/javascript src=/blog/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>