<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deeplearnings on</title><link>https://lizliu27.github.io/blog/deeplearning/</link><description>Recent content in Deeplearnings on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 14:22:17 +0800</lastBuildDate><atom:link href="https://lizliu27.github.io/blog/deeplearning/index.xml" rel="self" type="application/rss+xml"/><item><title>Seq2Seq, RNN, LSTM, Transformer</title><link>https://lizliu27.github.io/blog/deeplearning/transformer/</link><pubDate>Thu, 28 Mar 2024 14:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/transformer/</guid><description>Project 4 Implementing sequence-to-sequence (Seq2Seq) models and transformers for natural language processing tasks. A summary of the tasks and techniques covered:
RNN and LSTM Implementation Implementing a vanilla RNN unit using PyTorch Linear layers and activations. Implementing an LSTM unit using PyTorch nn.Parameter and activations, following a set of equations. Seq2Seq Implementation Implementing Seq2Seq models with an encoder and decoder. Seq2Seq with Attention Implementing a simple form of attention, using cosine similarity, to evaluate its impact on model performance.</description></item><item><title> Deep Learning in auto-diagnosis Electroencephalography</title><link>https://lizliu27.github.io/blog/deeplearning/eeg/</link><pubDate>Fri, 22 Dec 2023 14:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/eeg/</guid><description>This project was led by my awesome teammate zheng cheng. We together replicated a result an exsiting paper.
We tried to replicate the results and applied deep learning models to the auto-diagnosis of epilepsy via Electroencephalography (EEG) profiles.
We developed an architecture based on transformers, similar to those used for understanding sentiment in text. This architecture helped us extract important information from our data. We made several adjustments to this architecture and fine-tuned its settings.</description></item><item><title>CNN</title><link>https://lizliu27.github.io/blog/deeplearning/cnn/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/cnn/</guid><description>Project 2: Training Convolutional Neural Network (CNN) from scratch Module Implementation:
I learned to build a two-layer network with fully connected layers and a sigmoid activation function. I implemented a vanilla CNN with a convolutional layer, ReLU activation, max-pooling layer, and fully connected layer for classification. I had the opportunity to design and build my own custom CNN model, adhering to the principles of network architecture design. CNN Model Summary</description></item><item><title>Saliency Maps, GradCAM and Style Transfer</title><link>https://lizliu27.github.io/blog/deeplearning/saliency_map_style_transfer/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/saliency_map_style_transfer/</guid><description>Project 3: Implementing various techniques related to image analysis and manipulation using deep learning Specifically convolutional neural networks (CNNs) and style transfer. Here&amp;rsquo;s a summary of the tasks and techniques covered:
Class Model Visualizations: Synthesizing images to maximize classification scores for specific classes, providing insights into network focus during classification. Generating class-specific saliency maps to understand image areas influencing classification decisions. Creating fooling images by perturbing input images to mislead pretrained networks.</description></item><item><title>Traning Neural Networks</title><link>https://lizliu27.github.io/blog/deeplearning/mlp/</link><pubDate>Tue, 28 Nov 2023 11:22:17 +0800</pubDate><guid>https://lizliu27.github.io/blog/deeplearning/mlp/</guid><description>Project 1: Training Neural Networks for MNIST Recognition The first project focused on building a simple pipeline for training neural networks to recognize hand-written digits from the MNIST dataset. The pipeline implementation encompassed two neural network architectures, each equipped with functionalities to load data, train, and optimize the models.
Neural Network Architectures Two distinct neural network architectures were implemented from scratch for this project:
Simple Softmax Regression: Composed of a fully-connected layer followed by a ReLU activation.</description></item></channel></rss>